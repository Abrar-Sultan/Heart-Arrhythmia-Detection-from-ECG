{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "labeled-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acquired-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# read all files in train_data dir\n",
    "train_files = glob.glob('train_data/*.dat')\n",
    "print(len(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entire-crest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    N = Normal beat\\n    L = Left bundle branch block beat\\n    R = Right bundle branch block beat\\n    V = Premature ventricular contraction\\n    / = Paced beat(PB)\\n    A = Atrial premature beat\\n    f = Fusion of paced and normal beat\\n    F = Fusion of ventricular and normal beat\\n    j = Nodal (junctional) escape beat\\n    a = Aberrated atrial premature beat\\n    E = Ventricular escape beat\\n    J = Nodal (junctional) premature beat\\n    e = Atrial escape beat\\n    S = Supraventricular premature beat\\n   \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Annotation Meaning From https://archive.physionet.org/physiobank/annotations.shtml\n",
    "\"\"\"\n",
    "    N = Normal beat\n",
    "    L = Left bundle branch block beat\n",
    "    R = Right bundle branch block beat\n",
    "    V = Premature ventricular contraction\n",
    "    / = Paced beat(PB)\n",
    "    A = Atrial premature beat\n",
    "    f = Fusion of paced and normal beat\n",
    "    F = Fusion of ventricular and normal beat\n",
    "    j = Nodal (junctional) escape beat\n",
    "    a = Aberrated atrial premature beat\n",
    "    E = Ventricular escape beat\n",
    "    J = Nodal (junctional) premature beat\n",
    "    e = Atrial escape beat\n",
    "    S = Supraventricular premature beat\n",
    "   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interpreted-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to extract the corresponding heartbeat\n",
    "classes = ['N', 'L', 'R', 'E', '/', 'V', 'A', 'f', 'F', 'j', 'a', 'J', 'e', 'S']\n",
    "\n",
    "# map beat annotation to class type for saving image in floder NOR and ABNOR\n",
    "mapping = {'N': 'NOR', 'L': 'ABNOR', 'R': 'ABNOR', 'E': 'ABNOR', '/': 'ABNOR', 'V': 'ABNOR', 'A': 'ABNOR', 'f': 'ABNOR', \n",
    "           'F': 'ABNOR', 'j': 'ABNOR', 'a': 'ABNOR', 'J': 'ABNOR', 'e': 'ABNOR', 'S': 'ABNOR'}\n",
    "\n",
    "# image number\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compound-shadow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\\100.dat\n",
      "Completed record: train_data\\100\n",
      "train_data\\101.dat\n",
      "Completed record: train_data\\101\n",
      "train_data\\102.dat\n",
      "Completed record: train_data\\102\n",
      "train_data\\103.dat\n",
      "Completed record: train_data\\103\n",
      "train_data\\105.dat\n",
      "Completed record: train_data\\105\n",
      "train_data\\106.dat\n",
      "Completed record: train_data\\106\n",
      "train_data\\108.dat\n",
      "Completed record: train_data\\108\n",
      "train_data\\111.dat\n",
      "Completed record: train_data\\111\n",
      "train_data\\112.dat\n",
      "Completed record: train_data\\112\n",
      "train_data\\113.dat\n",
      "Completed record: train_data\\113\n",
      "train_data\\115.dat\n",
      "Completed record: train_data\\115\n",
      "train_data\\116.dat\n",
      "Completed record: train_data\\116\n",
      "train_data\\117.dat\n",
      "Completed record: train_data\\117\n",
      "train_data\\118.dat\n",
      "Completed record: train_data\\118\n",
      "train_data\\121.dat\n",
      "Completed record: train_data\\121\n",
      "train_data\\122.dat\n",
      "Completed record: train_data\\122\n",
      "train_data\\123.dat\n",
      "Completed record: train_data\\123\n",
      "train_data\\124.dat\n",
      "Completed record: train_data\\124\n",
      "train_data\\200.dat\n",
      "Completed record: train_data\\200\n",
      "train_data\\201.dat\n",
      "Completed record: train_data\\201\n",
      "train_data\\202.dat\n",
      "Completed record: train_data\\202\n",
      "train_data\\205.dat\n",
      "Completed record: train_data\\205\n",
      "train_data\\207.dat\n",
      "Completed record: train_data\\207\n",
      "train_data\\208.dat\n",
      "Completed record: train_data\\208\n",
      "train_data\\209.dat\n",
      "Completed record: train_data\\209\n",
      "train_data\\210.dat\n",
      "Completed record: train_data\\210\n",
      "train_data\\213.dat\n",
      "Completed record: train_data\\213\n",
      "train_data\\214.dat\n",
      "Completed record: train_data\\214\n",
      "train_data\\215.dat\n",
      "Completed record: train_data\\215\n",
      "train_data\\217.dat\n",
      "Completed record: train_data\\217\n",
      "train_data\\219.dat\n",
      "Completed record: train_data\\219\n",
      "train_data\\221.dat\n",
      "Completed record: train_data\\221\n",
      "train_data\\222.dat\n",
      "Completed record: train_data\\222\n",
      "train_data\\223.dat\n",
      "Completed record: train_data\\223\n",
      "train_data\\228.dat\n",
      "Completed record: train_data\\228\n",
      "train_data\\231.dat\n",
      "Completed record: train_data\\231\n",
      "train_data\\232.dat\n",
      "Completed record: train_data\\232\n",
      "train_data\\234.dat\n",
      "Completed record: train_data\\234\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for record in train_files:\n",
    "    print(record)\n",
    "    record = record[:-4]\n",
    "    signals, fields = wfdb.rdsamp(record, channels = [0])  \n",
    "    annotation = wfdb.rdann(record, 'atr')\n",
    "\n",
    "    # dict to store the segments for each class\n",
    "    segments = dict()\n",
    "    \n",
    "    # indices for all beats\n",
    "    beats = list(annotation.sample)\n",
    "\n",
    "    for beat in classes:\n",
    "        ids = np.in1d(annotation.symbol, beat)\n",
    "        imp_beats = annotation.sample[ids]\n",
    "\n",
    "        beat_samples = []\n",
    "\n",
    "        for i in imp_beats:\n",
    "            j = beats.index(i)\n",
    "            if j != 0 and j != len(beats) - 1:\n",
    "                \n",
    "                # beats before and after this beat\n",
    "                x = beats[j-1]\n",
    "\n",
    "                if x + 20 > beats[j]:\n",
    "                    l = 2\n",
    "                    while (x + 20 > beats[j]) and (j-l >= 0):\n",
    "                        x = beats[j-l]\n",
    "                        l -= 1\n",
    "\n",
    "                    if j-l < 0:\n",
    "                        continue\n",
    "                    \n",
    "                y = beats[j+1]\n",
    "                if y - 20 < beats[j]:\n",
    "                    l = 2\n",
    "                    while (y - 20 < beats[j]) and (j+l < len(beats) - 1):\n",
    "                        y = beats[j+l]\n",
    "                        l += 1\n",
    "\n",
    "                    if j+l == len(beats) - 1:\n",
    "                        continue\n",
    "\n",
    "                # 20 sec after and before above peaks x,y \n",
    "                start = x + 20\n",
    "                end = y - 20\n",
    "\n",
    "                # centre the peak\n",
    "                if abs(beats[j] - start) < abs(beats[j] - end):\n",
    "                    end = beats[j] + abs(beats[j] - start)\n",
    "                else:\n",
    "                    start = beats[j] - abs(beats[j] - end)\n",
    "                \n",
    "                beat_samples.append(signals[start: end, 0])\n",
    "        \n",
    "        segments[beat] = beat_samples\n",
    "    \n",
    "    for key in segments.keys():\n",
    "        if not segments[key]:\n",
    "            continue\n",
    "            \n",
    "        val = segments[key]\n",
    "        directory = 'Train/' + mapping[key]\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        for i in val:\n",
    "            fig = plt.figure(frameon=False)\n",
    "            plt.plot(i)\n",
    "            plt.xticks([]), plt.yticks([])\n",
    "            for spine in plt.gca().spines.values():\n",
    "                spine.set_visible(False)\n",
    "        \n",
    "            filename = directory + '/' + str(count + 1) + '.png'\n",
    "            count += 1\n",
    "            fig.savefig(filename)\n",
    "            plt.close(fig=fig)\n",
    "\n",
    "            im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "            im_gray = cv2.resize(im_gray, (234, 234), interpolation = cv2.INTER_LANCZOS4)\n",
    "            cv2.imwrite(filename, im_gray)\n",
    "    \n",
    "    print('Completed record: ' + record)\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-lesbian",
   "metadata": {},
   "source": [
    "# Test Data Peparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "common-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# read all files in test_data dir\n",
    "test_files = glob.glob('test_data/*.dat')\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "three-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classes to extract the corresponding heartbeat\n",
    "classes = ['N', 'L', 'R', 'E', '/', 'V', 'A', 'f', 'F', 'j', 'a', 'J', 'e', 'S']\n",
    "\n",
    "# map beat annotation to class type for saving image in floder NOR and ABNOR\n",
    "mapping = {'N': 'NOR', 'L': 'ABNOR', 'R': 'ABNOR', 'E': 'ABNOR', '/': 'ABNOR', 'V': 'ABNOR', 'A': 'ABNOR', 'f': 'ABNOR', \n",
    "           'F': 'ABNOR', 'j': 'ABNOR', 'a': 'ABNOR', 'J': 'ABNOR', 'e': 'ABNOR', 'S': 'ABNOR'}\n",
    "\n",
    "# image number\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-marble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data\\104.dat\n",
      "Completed record: test_data\\104\n",
      "test_data\\107.dat\n",
      "Completed record: test_data\\107\n",
      "test_data\\109.dat\n",
      "Completed record: test_data\\109\n",
      "test_data\\114.dat\n",
      "Completed record: test_data\\114\n",
      "test_data\\119.dat\n",
      "Completed record: test_data\\119\n",
      "test_data\\203.dat\n",
      "Completed record: test_data\\203\n",
      "test_data\\212.dat\n",
      "Completed record: test_data\\212\n",
      "test_data\\220.dat\n",
      "Completed record: test_data\\220\n",
      "test_data\\230.dat\n",
      "Completed record: test_data\\230\n",
      "test_data\\233.dat\n",
      "Completed record: test_data\\233\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for record in test_files:\n",
    "    print(record)\n",
    "    record = record[:-4]\n",
    "    signals, fields = wfdb.rdsamp(record, channels = [0])  \n",
    "    annotation = wfdb.rdann(record, 'atr')\n",
    "\n",
    "    # dict to store the segments for each class\n",
    "    segments = dict()\n",
    "    \n",
    "    # indices for all beats\n",
    "    beats = list(annotation.sample)\n",
    "\n",
    "    for beat in classes:\n",
    "        ids = np.in1d(annotation.symbol, beat)\n",
    "        imp_beats = annotation.sample[ids]\n",
    "\n",
    "        beat_samples = []\n",
    "\n",
    "        for i in imp_beats:\n",
    "            j = beats.index(i)\n",
    "            if j != 0 and j != len(beats) - 1:\n",
    "                \n",
    "                # beats before and after this beat\n",
    "                x = beats[j-1]\n",
    "\n",
    "                if x + 20 > beats[j]:\n",
    "                    l = 2\n",
    "                    while (x + 20 > beats[j]) and (j-l >= 0):\n",
    "                        x = beats[j-l]\n",
    "                        l -= 1\n",
    "\n",
    "                    if j-l < 0:\n",
    "                        continue\n",
    "                    \n",
    "                y = beats[j+1]\n",
    "                if y - 20 < beats[j]:\n",
    "                    l = 2\n",
    "                    while (y - 20 < beats[j]) and (j+l < len(beats) - 1):\n",
    "                        y = beats[j+l]\n",
    "                        l += 1\n",
    "\n",
    "                    if j+l == len(beats) - 1:\n",
    "                        continue\n",
    "\n",
    "                # 20 sec after and before above peaks x,y\n",
    "                start = x + 20\n",
    "                end = y - 20\n",
    "\n",
    "                # centre the peak\n",
    "                if abs(beats[j] - start) < abs(beats[j] - end):\n",
    "                    end = beats[j] + abs(beats[j] - start)\n",
    "                else:\n",
    "                    start = beats[j] - abs(beats[j] - end)\n",
    "                \n",
    "                beat_samples.append(signals[start: end, 0])\n",
    "        \n",
    "        segments[beat] = beat_samples\n",
    "    \n",
    "    for key in segments.keys():\n",
    "        if not segments[key]:\n",
    "            continue\n",
    "            \n",
    "        val = segments[key]\n",
    "        directory = 'Test/' + mapping[key]\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        for i in val:\n",
    "            fig = plt.figure(frameon=False)\n",
    "            plt.plot(i)\n",
    "            plt.xticks([]), plt.yticks([])\n",
    "            for spine in plt.gca().spines.values():\n",
    "                spine.set_visible(False)\n",
    "        \n",
    "            filename = directory + '/' + str(count + 1) + '.png'\n",
    "            count += 1\n",
    "            fig.savefig(filename)\n",
    "            plt.close(fig=fig)\n",
    "\n",
    "            im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "            im_gray = cv2.resize(im_gray, (234, 234), interpolation = cv2.INTER_LANCZOS4)\n",
    "            cv2.imwrite(filename, im_gray)\n",
    "    \n",
    "    print('Completed record: ' + record)\n",
    "        \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
